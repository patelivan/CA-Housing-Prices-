---
title: "Linear Regression"
author: "Ivan Patel"
date: "10/1/2020"
output: 
  html_document: 
    df_print: kable

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=F, message=F}
library(tidyverse)
library(dplyr)
library(skimr)
library(sf)
library(mapview)
library(readxl)
library(moderndive)
library(broom)
```

Before proceeding with the linear regression, all the housing values exceeding $1,000,000 were changed to 
$1,000,000. I did this because linear regression is very sensitive to outliers. 

```{r, echo=F}
# Read the model data 
model_data<- st_read('/Users/ivanpatel/Desktop/School/Senior Project/Data/Model/model.shp')

names(model_data) <- c("C_GEOID", "T_GEOID", "NAME_tr",                           
                       "year","NAME_county", "Coastal", 
                       "median.housing.value.estimate",                           
                       "population.estimate", "median.age.estimate",              
                       "median.number.rooms.estimate",                       
                       "married_couple_family_1_unit_structures.estimate",       
                       "married_couple_family_2_or_more_unit_structures.estimate",
                       "aggregate_travel_time_to_work_minutes.estimate", 
                       "median.county.household.income.estimate", 
                       "renter_occupied_perc", 
                       "geometry")
```


```{r, echo=F}
# Edit ca_final to build the model data -----------------------------------

model_data$median.housing.value.estimate[model_data$median.housing.value.estimate > 1000000] <- 1000000

ols_model<- model_data %>% 
  mutate(log_house_values_estimate = log(median.housing.value.estimate), 
         log_county_income_estimate = log(median.county.household.income.estimate)) %>%
  st_drop_geometry() %>% 

# Remove the county names, tract name, the GEOID's and geometry  st_drop_geometry() %>% 
  select(-C_GEOID:-NAME_county, 
         -married_couple_family_2_or_more_unit_structures.estimate, 
         -married_couple_family_1_unit_structures.estimate, 
         -aggregate_travel_time_to_work_minutes.estimate, 
         -median.housing.value.estimate, 
         -median.county.household.income.estimate, 
         -population.estimate)

# Let's check which variables the model data contains
ols_model %>% names()
```

```{r, echo=F}
model <- lm(data = ols_model, log_house_values_estimate ~ . + 
              renter_occupied_perc * Coastal +
              log_county_income_estimate * Coastal +
              log_county_income_estimate * renter_occupied_perc)

# Regression Results
get_regression_table(model)
get_regression_summaries(model)

# Correlation between the variables - Hide lower triangle
```


```{r, echo=F}
mcor <- ols_model %>% select(median.age.estimate:log_county_income_estimate) %>% cor() %>% round(2)
mcor[lower.tri(mcor, diag=T)] <- ''
mcor <- as.data.frame(mcor)
mcor
```


```{r}
model_predictions <- get_regression_points(model) %>% select(ID, log_house_values_estimate,log_house_values_estimate_hat, residual) 

par(mfrow = c(1,2))
plot(density(model_predictions$log_house_values_estimate), frame = F, main = 'Actual House Values')
plot(density(model_predictions$log_house_values_estimate_hat), frame = F, main = 'Predicted House Values')

```

## Checking for potential problems

One key assumption of linear regression is that the mean of residuals is zero. 
Since the mean of residuals is approximately zero, this assumption holds true for this model.

```{r}
mean(model_predictions$residual)
```

#### Non-linearity of the Data

Residual plots are a useful graphical tool for identifying non-linearity. It will show no fitted discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model.

There appears to be a little pattern in the residuals below which suggests that the linear model is a good fit to the data. The red line through the scatterplot is also straight and horizontal. Thus, the linearity assumption is satisfied.

```{r, echo=F}
plot(model, which = 1)
```

#### Do the residuals follow a Normal Distribution?

Using a QQ-plot, we can compare the residuals to “ideal” normal observations along the 45-degree line.

There are several points below -2 with large residuals, and R automatically flagged them. Later, I identify what are those observations and consider whether or not we should remove them.  

But a lot of points are also right on the line. DO you think it is fair to assume that normality holds here?

```{r, echo=F}
plot(model, which = 2)
```

#### Non-constant Variance of Error Terms

The third plot is a scale-location plot (square rooted standardized residual vs. predicted value). This is useful for checking the assumption of homoscedasticity. 

If the red line you see on your plot is flat and horizontal with equally and randomly spread data points, we have not violated the assumption.

```{r, echo=F}
plot(model, which = 3)
```


I think the biggest problem is influential points. Some terminology: 

A data point has high leverage, if it has extreme predictor x values. This can be detected by examining the leverage statistic or the hat-value. A value of this statistic above 
2(p + 1)/n indicates an observation with high leverage, where, p is the number of predictors and n is the number of observations.

An influential value is a value, which inclusion or exclusion can alter the results of the regression analysis. Such a value is associated with a large residual.

A rule of thumb is that an observation has high influence if Cook’s distance exceeds 4/(n - p - 1), where n is the number of observations and p the number of predictor variables.


```{r, echo=F}
plot(model, which = 5)
```

When data points have high Cook’s distance scores and are to the upper or lower right of the leverage plot, they have leverage meaning they are influential to the regression results.

On the above plot, outlying values are not located at the upper right corner or at the lower
right corner, which is a good thing. But, the regression results will be altered if we exclude those cases. 
 
```{r}
model.diag.metrics <- augment(model)
model.diag.metrics %>%
  top_n(3, wt = .cooksd)

```




